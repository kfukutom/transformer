**NLP Personal Project**

Use [START] and [END] tokens, create a full transformer architecture. 
Coded up a transformer neural network components piece by piece, vectors included. 

In this study, we endeavor to architect and develop an advanced computational model predicated upon the Transformer neural network architecture. This model is meticulously engineered and trained to discern and identify nuanced semantic linguistic characteristics within translated texts, with a primary focus on Japanese. Specifically, the corpus for training comprises business-related articles sourced from Japanese publications. The objective of this research is to harness the sophisticated capabilities of Transformer-based models for semantic analysis, enabling private hedge funds and equity firms engaged in financial capitalization to derive novel insights and make informed decisions grounded in the semantic nuances of Japanese business discourse as presented in newspaper articles.

This endeavor not only contributes to the burgeoning field of natural language processing (NLP) but also aims to bridge the gap between advanced computational linguistic analysis and practical financial applications. By focusing on the detection of semantic linguistic traits in Japanese business articles, the study seeks to provide equity-based firms and hedge funds with a competitive edge, leveraging the subtleties of language semantics to forecast market trends and corporate strategies more accurately. The application of Transformer architecture models in this context represents a significant advancement in the application of artificial intelligence (AI) within the domain of financial analysis, offering a novel approach to understanding the complex interplay between language, culture, and market dynamics.
