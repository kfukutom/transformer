{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer import Transformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.28114237  1.44752402 -0.60824052  0.40310601  1.43924712 -1.38296083\n",
      "   0.95163263  1.51139518]\n",
      " [ 0.14419632  1.60078477 -0.55843829  0.20580793  0.69943993 -0.27722628\n",
      "   0.05123652 -0.4286159 ]\n",
      " [-0.63881402 -0.63938131 -0.83124101 -0.86264979 -0.62493197 -0.30883055\n",
      "  -1.33794331 -0.91399712]\n",
      " [-0.767213    1.4008975  -1.24628059 -0.04933137 -0.16553654  0.47415481\n",
      "   1.08705046 -0.68796479]]\n",
      "K\n",
      " [[-1.34879403 -0.64603967 -0.46963968 -1.14166133 -0.16892808 -0.27302121\n",
      "  -0.97135169 -0.2109268 ]\n",
      " [ 0.26196293  0.31645742  0.69783567  0.77294645 -0.57830048 -1.99095362\n",
      "   0.30677922  0.26174165]\n",
      " [-0.34083638 -1.47527041  0.17580892 -0.7577799  -0.36667739 -0.93340931\n",
      "   0.17661323 -1.01089128]\n",
      " [ 0.93246473  0.03781071  1.82269559  1.04783508 -1.74464269 -0.78567752\n",
      "   0.27887145 -1.03483974]]\n",
      "V\n",
      " [[ 1.74010932  0.32677542  1.7065823   0.42829971 -1.24734848  0.22574623\n",
      "  -0.13892477 -1.37359953]\n",
      " [ 0.41882027 -1.74923582  1.38457268  0.66992071 -0.29896037 -0.59319946\n",
      "  -0.09752715  1.14305372]\n",
      " [ 0.15505064 -0.95972554  0.2730859   0.48775724 -0.2861629   0.01688586\n",
      "  -0.07128753 -1.2253131 ]\n",
      " [ 2.43990161 -1.86603397 -0.08189239  1.59457026 -1.05406968 -0.83403876\n",
      "  -0.47583429 -0.01642862]]\n"
     ]
    }
   ],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "\n",
    "print(\"Q\\n\", q) # queue\n",
    "print(\"K\\n\", k) # key\n",
    "print(\"V\\n\", v) # value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.94642711,  3.28944871, -3.58120516, -2.15997606],\n",
       "       [-1.20318828,  0.36472455, -2.2202434 , -1.15185057],\n",
       "       [ 4.33221773, -1.28995282,  2.87361742, -1.13321048],\n",
       "       [-0.24088914, -1.36036143, -1.48137036, -2.05436141]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention matrix will have every word look at every other word in the sentence\n",
    "# 4 cross 4 matrix for example \"My name is Ken\" len(4)\n",
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8283162637187776, 0.7187877631935535, 0.6445927040445486)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.matmul(q, k.T).var()\n",
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var() #minimize skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.39527268,  1.16299574, -1.26614723, -0.76366686],\n",
       "       [-0.42539129,  0.1289496 , -0.78497458, -0.40724067],\n",
       "       [ 1.53167027, -0.45606719,  1.01597718, -0.40065041],\n",
       "       [-0.08516717, -0.4809604 , -0.52374351, -0.72632644]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking ##\n",
    "- This is to ensure words don't get context from words generated in the future\n",
    "- Not required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L) )) # triangular matrix\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.39527268,        -inf,        -inf,        -inf],\n",
       "       [-0.42539129,  0.1289496 ,        -inf,        -inf],\n",
       "       [ 1.53167027, -0.45606719,  1.01597718,        -inf],\n",
       "       [-0.08516717, -0.4809604 , -0.52374351, -0.72632644]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmask, convert a vector into a probability distribution\n",
    "def softmax(x): \n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=1)).T\n",
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.36485788, 0.63514212, 0.        , 0.        ],\n",
       "       [0.57667078, 0.07900681, 0.34432241, 0.        ],\n",
       "       [0.35152087, 0.23662482, 0.22671478, 0.18513954]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.74010932,  0.32677542,  1.7065823 ,  0.42829971, -1.24734848,\n",
       "         0.22574623, -0.13892477, -1.37359953],\n",
       "       [ 0.90090299, -0.99178677,  1.50206042,  0.58176339, -0.64498724,\n",
       "        -0.29440068, -0.1126314 ,  0.22483296],\n",
       "       [ 1.08994727, -0.28021472,  1.18755641,  0.46786198, -0.84146163,\n",
       "         0.08912864, -0.11236506, -1.12370844],\n",
       "       [ 1.19766254, -0.86210486,  0.97427464,  0.71487593, -0.76923779,\n",
       "        -0.21159648, -0.17616998, -0.49321219]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v\n",
    "# this new matricies better encapsulates the context of the masked words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New V\n",
      " [[ 0.70361206 -1.5865436   1.16601762  0.74611    -0.43798238 -0.53058441\n",
      "  -0.1402274   0.70648047]\n",
      " [ 1.13572215 -1.18650689  0.94771752  0.79844007 -0.68226004 -0.36894606\n",
      "  -0.18913867 -0.05737777]\n",
      " [ 1.1939916  -0.40243774  1.08971685  0.55470018 -0.85784786  0.01797784\n",
      "  -0.14037853 -1.03836764]\n",
      " [ 1.19766254 -0.86210486  0.97427464  0.71487593 -0.76923779 -0.21159648\n",
      "  -0.17616998 -0.49321219]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.36485788 0.63514212 0.         0.        ]\n",
      " [0.57667078 0.07900681 0.34432241 0.        ]\n",
      " [0.35152087 0.23662482 0.22671478 0.18513954]]\n"
     ]
    }
   ],
   "source": [
    "# MASK is set to true when we are decoding\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v) # vector\n",
    "    return out, attention\n",
    "\n",
    "\"\"\"print(\"Q\\n\", q) # queue\n",
    "print(\"K\\n\", k) # key\n",
    "print(\"V\\n\", v) # value\"\"\"\n",
    "#apply scaled product def\n",
    "values, attnetion = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
